{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8af00ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import email\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bdd51",
   "metadata": {},
   "source": [
    "## Load the Data ##\n",
    "Two datasets will be used, the Enron email corpus, for not-spam emails, and a collection of \"419\" fraudulent emails, which are spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a761bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath1 = \"emails.csv\"\n",
    "filepath2 = \"fraudulent_emails.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5973643d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 517401 rows and 2 columns!\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "# Load emails from Enron email corpus\n",
    "emails = pd.read_csv(filepath1)\n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0],emails.shape[1]))\n",
    "print(emails.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9f04a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates and extracts bodies of messages from header information\n",
    "def extract_messages(df):\n",
    "    messages = []\n",
    "    for msg in df[\"message\"]:\n",
    "        e = email.message_from_string(msg)\n",
    "        msg_body = e.get_payload()\n",
    "        messages.append(msg_body)\n",
    "    print(\"Success!\")\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f55390bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "                                                   0\n",
      "0                          Here is our forecast\\n\\n \n",
      "1  Traveling to have a business meeting takes the...\n",
      "2                     test successful.  way to go!!!\n",
      "3  Randy,\\n\\n Can you send me a schedule of the s...\n",
      "4                Let's shoot for Tuesday at 11:45.  \n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with email bodies\n",
    "bodies = extract_messages(emails)\n",
    "bodies_df = pd.DataFrame(bodies)\n",
    "print(bodies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d546a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3978 spam emails.\n"
     ]
    }
   ],
   "source": [
    "# Load fraudulent (spam) emails\n",
    "with open(filepath2, 'r', encoding=\"latin1\") as file:\n",
    "    data = file.read()\n",
    "    \n",
    "fraud_emails = data.split(\"From r\")\n",
    "\n",
    "print(\"Successfully loaded {} spam emails.\".format(len(fraud_emails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7674c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "                                                   0\n",
      "0  FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-2...\n",
      "1  Dear Friend,\\n\\nI am Mr. Ben Suleman a custom ...\n",
      "2  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...\n",
      "3  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF EL...\n",
      "4  Dear sir, \\n \\nIt is with a heart full of hope...\n"
     ]
    }
   ],
   "source": [
    "# Convert fraudulent email data into a pandas DataFrame\n",
    "fraud_bodies = extract_messages(pd.DataFrame(fraud_emails, columns=[\"message\"], dtype=str))\n",
    "fraud_bodies_df = pd.DataFrame(fraud_bodies[1:]) # DataFrame with just the message bodies (no header)\n",
    "print(fraud_bodies_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791351c",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##\n",
    "We convert the emails into sample of roughly equal size and take `Nsamp` emails from each category. Every sample will be `maxtokens` length and each token will be no longer than `maxtokenlen`. We also perform the following steps:\n",
    "1. Tokenization\n",
    "2. Punctuation removal\n",
    "3. Lowercasing\n",
    "2. Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "facedb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of emails\n",
    "Nsamp = 1000 # number of samples to generate in each class (spam and non-spam)\n",
    "maxtokens = 50 # maximum number of tokens in each email\n",
    "maxtokenlen = 20 # max length of each token\n",
    "\n",
    "def tokenize(row):\n",
    "    if row in (None, ''):\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = str(row).split()[:maxtokens] # Split on whitespace\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68ea1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: remove punctuation, and uppercase\n",
    "#             remove stop words\n",
    "def reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower()\n",
    "            token = re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token = token[:maxtokenlen]\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8569aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset of emails by performing all preprocessing for the two sets of email data\n",
    "EnronEmails = bodies_df.iloc[:,0].apply(tokenize)\n",
    "EnronEmails = EnronEmails.apply(stop_word_removal)\n",
    "EnronEmails = EnronEmails.apply(reg_expressions)\n",
    "EnronEmails = EnronEmails.sample(Nsamp)\n",
    "\n",
    "SpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\n",
    "SpamEmails = SpamEmails.apply(stop_word_removal)\n",
    "SpamEmails = SpamEmails.apply(reg_expressions)\n",
    "SpamEmails = SpamEmails.sample(Nsamp)\n",
    "\n",
    "# Convert the data to a single dataset by concatenating the two sets\n",
    "raw_data = pd.concat([SpamEmails, EnronEmails], axis=0).to_numpy() # convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "645bfd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined data represented as numpy array is:\n",
      "(2000,)\n",
      "Data represented as numpy array is\n",
      "[list(['attnbrbrmy', 'name', 'is', 'kelvin', 'brown', 'i', 'am', 'from', 'british', 'and', 'also', 'a', 'special', 'adviser', 'to', 'the', 'former', 'liberian', 'president', 'in', 'africa', 'mr', 'charles', 'taylor', 'before', 'he', 'left', 'office', 'he', 'instructed', 'me', 'being', 'his', 'special', 'adviser', 'to', 'look', 'for', 'capable', 'hand', 'some', 'one', 'who', 'will', 'invest', 'his', 'money', '', 'fifteen', 'million'])\n",
      " list(['emailmessagemessage', 'object', 'xfbdeb', 'emailmessagemessage', 'object', 'xfbddab', 'emailmessagemessage', 'object', 'xfbde'])\n",
      " list(['emailmessagemessage', 'object', 'xfbd', 'emailmessagemessage', 'object', 'xfbde'])\n",
      " ...\n",
      " list(['if', 'consenting', 'corporate', 'action', 'habitually', 'yes', 'original', 'message', 'from', 'haedicke', 'mark', 'sent', 'tuesday', 'may', '', '', '', 'pm', 'to', 'douglas', 'stephen', 'h', 'subject', 're', 'canadian', 'retail', 'is', 'signing', 'unanimous', 'consent', 'houston', 'issue', 'mark', 'stephen', 'h', 'douglasenronenronxga', '', '', 'pm', 'to', 'peter', 'keohanecalectect'])\n",
      " list(['', 'works', 'me', 'the', 'best', 'way', 'get', 'house', 'house', 'go', 'shepherd', 'past', '', 'make', 'right', 'sunset', 'blvd', 'keep', 'going', 'past', 'kirby', 'buffalo', 'speedway', 'couple', 'stop', 'signs', 'my', 'house', 'right', 'it'])\n",
      " list(['chris', 'the', 'enaim', 'tp', 'east', 'book', 'set', 'internal', 'counterparty', 'desktodesk', 'trading', 'enrononline', 'the', 'following', 'user', 'id', 'password', 'give', 'access', 'live', 'prices', 'website', 'httpwwwenrononlineco', 'user', 'id', 'adm', 'password', 'welcome', 'note', 'case', 'sensitive', 'please', 'keep', 'user'])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of combined data represented as numpy array is:\")\n",
    "print(raw_data.shape)\n",
    "print(\"Data represented as numpy array is\")\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d8622b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Create headers corresponding to these emails \n",
    "Categories = ['spam','notspam']\n",
    "\n",
    "# Create a list of size 2*Nsamp\n",
    "header = ([1]*Nsamp)\n",
    "header.extend(([0]*Nsamp))\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2728f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling bag of words\n",
    "def assemble_bow(data):\n",
    "    used_tokens = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for item in data:\n",
    "        for token in item:\n",
    "            if token in all_tokens:\n",
    "                if token not in used_tokens:\n",
    "                    used_tokens.append(token)\n",
    "            else:\n",
    "                all_tokens.append(token)\n",
    "                \n",
    "    df = pd.DataFrame(0, index = np.arange(len(data)), columns = used_tokens)\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        for token in item:\n",
    "            if token in used_tokens:\n",
    "                df.iloc[i][token] += 1\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f55893f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      he  special  adviser  to  his  emailmessagemessage  object  xfbde  i  \\\n",
      "0      2        2        2   2    2                    0       0      0  1   \n",
      "1      0        0        0   0    0                    3       3      1  0   \n",
      "2      0        0        0   0    0                    2       2      1  0   \n",
      "3      0        0        0   0    0                    0       0      0  1   \n",
      "4      0        0        0   0    0                    0       0      0  2   \n",
      "...   ..      ...      ...  ..  ...                  ...     ...    ... ..   \n",
      "1995   0        0        0   0    0                    0       0      0  0   \n",
      "1996   0        0        0   0    0                    0       0      0  0   \n",
      "1997   0        0        0   2    0                    0       0      0  0   \n",
      "1998   0        0        0   0    0                    0       0      0  0   \n",
      "1999   0        0        0   0    0                    0       0      0  0   \n",
      "\n",
      "      federal  ...  iso  noticedoc  feature  picks  jobs  robertshouectect  \\\n",
      "0           0  ...    0          0        0      0     0                 0   \n",
      "1           0  ...    0          0        0      0     0                 0   \n",
      "2           0  ...    0          0        0      0     0                 0   \n",
      "3           2  ...    0          0        0      0     0                 0   \n",
      "4           0  ...    0          0        0      0     0                 0   \n",
      "...       ...  ...  ...        ...      ...    ...   ...               ...   \n",
      "1995        0  ...    0          0        0      0     0                 0   \n",
      "1996        0  ...    0          0        0      0     0                 0   \n",
      "1997        0  ...    0          0        0      0     0                 0   \n",
      "1998        0  ...    0          0        0      0     0                 0   \n",
      "1999        0  ...    0          0        0      0     0                 0   \n",
      "\n",
      "      pdo  sherry  retail  kirby  \n",
      "0       0       0       0      0  \n",
      "1       0       0       0      0  \n",
      "2       0       0       0      0  \n",
      "3       0       0       0      0  \n",
      "4       0       0       0      0  \n",
      "...   ...     ...     ...    ...  \n",
      "1995    0       1       0      0  \n",
      "1996    0       0       0      0  \n",
      "1997    0       0       1      0  \n",
      "1998    0       0       0      1  \n",
      "1999    0       0       0      0  \n",
      "\n",
      "[2000 rows x 4921 columns]\n"
     ]
    }
   ],
   "source": [
    "# EnronSpamBag is a dataframe where columns are all unique words, rows are email samples\n",
    "# EnronSpamBag contains information about the occurrence of each word\n",
    "EnronSpamBag = assemble_bow(raw_data)\n",
    "print(EnronSpamBag)\n",
    "predictors = [column for column in EnronSpamBag.columns] # A list containing all unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6001f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to shuffle the data before we split it into training and test sets\n",
    "data, headers = shuffle(EnronSpamBag.values, header, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "89e4511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2000 samples in the dataset.\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", data.shape[0], \"samples in the dataset.\")\n",
    "slice = int(0.7*data.shape[0])\n",
    "train_data = data[:slice]\n",
    "train_target = headers[:slice]\n",
    "test_data = data[slice:]\n",
    "test_target = headers[slice:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6a137",
   "metadata": {},
   "source": [
    "### IMDB Dataset ###\n",
    "Next, we load the IMDB Moview Review Dataset, which contains pre-tagged positive and negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtokens = 200 # the maximum number of tokens per document\n",
    "maxtokenlen = 100 # the maximum length of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2337ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    imdb_data, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                  text = reader.read()\n",
    "            text = tokenize(text)\n",
    "            text = stop_word_removal(text)\n",
    "            text = reg_expressions(text)\n",
    "            imdb_data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np = np.array(imdb_data, dtype=object)\n",
    "    imdb_data, sentiments = shuffle(data_np, sentiments)\n",
    "    \n",
    "    return imdb_data, sentiments\n",
    "\n",
    "train_path = os.path.join('aclImdb', 'train')\n",
    "test_path = os.path.join('aclImdb', 'test')\n",
    "raw_data, raw_header = load_data(train_path)\n",
    "\n",
    "# raw_data is an numpt dtype array of variously sized lists, each containing an imdb review\n",
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5ab29cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'favorite',\n",
       " 'movie',\n",
       " 'what',\n",
       " 'great',\n",
       " 'story',\n",
       " 'really',\n",
       " 'was',\n",
       " 'id',\n",
       " 'like',\n",
       " 'able',\n",
       " 'buy',\n",
       " 'copy',\n",
       " 'seem',\n",
       " 'possible']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the tokens in the first review\n",
    "raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "94000a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample required number of samples\n",
    "random_indices = np.random.choice(range(len(raw_header)), size=(Nsamp*2, ), replace=False)\n",
    "random_indices = random_indices.tolist()\n",
    "\n",
    "imdb_dataset = [raw_data[i] for i in random_indices]\n",
    "imdb_headers = [raw_header[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "729ce34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ive', 'rarely', 'annoyed', 'leading', 'performance', 'i', 'ali', 'mcgraws', 'movie', 'god', 'bothersome', 'what', 'she', 'says', 'everything', 'tone', 'horrible', 'horrible', 'fact', 'that', 'contrast', 'ryan', 'oneal', 'brilliant', 'br', 'br', 'there', 'much', 'story', 'hes', 'rich', 'wooden', 'sacrifice', 'a', 'lot', 'love', 'his', 'father', 'stonewall', 'jackson', 'called', 'first', 'name', 'case', 'notice', 'difference', 'the', 'two', 'them', 'they', 'overcame', 'name', 'love', 'br', 'br', 'the', 'oscar', 'nominations', 'movie', 'indicate', 'bad', 'year', 'john', 'marley', 'fine', 'woodens', 'father', 'supporting', 'nomination', 'at', 'least', 'ali', 'win', 'br', 'br', 'i', 'still', 'think', 'katharine', 'ross', 'played', 'jennifer', 'again', 'me', 'katharine', 'ross', 'would', 'lot', 'movies', 'shes', 'certainly', 'better', 'actress', 'mcgraw', 'br', 'br', 'i', 'even', 'cry', 'got', 'sick', 'never', 'occured', 'even', 'feel', 'sad', 'br', 'br', 'it', 'nice', 'see', 'tommy', 'lee', 'jones', 'looking', 'like']\n"
     ]
    }
   ],
   "source": [
    "print(imdb_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "058013f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments and their frequencies:\n",
      "[0 1]\n",
      "[1024  976]\n"
     ]
    }
   ],
   "source": [
    "# Check the balance of the resulting data to ensure we haven't overwhelmingly selected from one of the labels\n",
    "# The data should be roughly split 50/50\n",
    "unique_elements, counts_elements = np.unique(imdb_headers, return_counts=True)\n",
    "print(\"Sentiments and their frequencies:\")\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "21203863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      horrible  br  name  love  the  movie  father  ali  i  katharine  ...  \\\n",
      "0            2  10     2     2    2      2       2    2  3          2  ...   \n",
      "1            0   0     0     0    1      0       0    0  0          0  ...   \n",
      "2            0   1     0     0    2      0       0    0  7          0  ...   \n",
      "3            0   0     0     0    1      0       0    0  1          0  ...   \n",
      "4            0   1     0     0    1      2       0    0  0          0  ...   \n",
      "...        ...  ..   ...   ...  ...    ...     ...  ... ..        ...  ...   \n",
      "1995         0   5     0     0    0      0       0    0  5          0  ...   \n",
      "1996         0   0     0     0    0      2       0    0  2          0  ...   \n",
      "1997         0   0     0     0    0      1       0    0  1          0  ...   \n",
      "1998         0   0     0     0    0      0       0    0  8          0  ...   \n",
      "1999         0   2     0     0    0      5       0    0  4          0  ...   \n",
      "\n",
      "      studded  ranging  gleeson  cartoonsbr  selfawareness  sparks  hosts  \\\n",
      "0           0        0        0           0              0       0      0   \n",
      "1           0        0        0           0              0       0      0   \n",
      "2           0        0        0           0              0       0      0   \n",
      "3           0        0        0           0              0       0      0   \n",
      "4           0        0        0           0              0       0      0   \n",
      "...       ...      ...      ...         ...            ...     ...    ...   \n",
      "1995        0        0        0           1              0       0      0   \n",
      "1996        0        0        0           0              0       0      0   \n",
      "1997        0        0        0           0              1       0      0   \n",
      "1998        0        0        0           0              0       1      1   \n",
      "1999        0        0        0           0              0       0      0   \n",
      "\n",
      "      rolebr  minded  moody  \n",
      "0          0       0      0  \n",
      "1          0       0      0  \n",
      "2          0       0      0  \n",
      "3          0       0      0  \n",
      "4          0       0      0  \n",
      "...      ...     ...    ...  \n",
      "1995       0       0      0  \n",
      "1996       0       0      0  \n",
      "1997       0       0      0  \n",
      "1998       0       0      0  \n",
      "1999       1       1      1  \n",
      "\n",
      "[2000 rows x 11690 columns]\n"
     ]
    }
   ],
   "source": [
    "BagOfReviews = assemble_bow(imdb_dataset)\n",
    "print(BagOfReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dcf1fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2000 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Split the DataFrame into training and test sets\n",
    "print(\"There are\", BagOfReviews.shape[0], \"samples in the dataset.\")\n",
    "slice = int(0.7*BagOfReviews.shape[0])\n",
    "data = BagOfReviews.values\n",
    "\n",
    "imdb_train_data = data[:slice]\n",
    "imdb_train_target = imdb_headers[:slice]\n",
    "imdb_test_data = data[slice:]\n",
    "imdb_test_target = imdb_headers[slice:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d5421",
   "metadata": {},
   "source": [
    "## Generalized Linear Models ##\n",
    "We will use logistic regression and Singular Value Decomposition (SVD) to build a classifier for our email and movie datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0368c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for logistic regression on the email dataset is 0.9816666666666667\n"
     ]
    }
   ],
   "source": [
    "email_model = LogisticRegression(max_iter=5000)\n",
    "email_model.fit(train_data, train_target)\n",
    "predicted = email_model.predict(test_data)\n",
    "accuracy = accuracy_score(test_target, predicted)\n",
    "print(\"The accuracy score for logistic regression on the email dataset is\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fce49131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for logistic regression on the IMDB dataset is 0.795\n"
     ]
    }
   ],
   "source": [
    "movie_model = LogisticRegression(max_iter=1000)\n",
    "movie_model.fit(imdb_train_data, imdb_train_target)\n",
    "predicted = movie_model.predict(imdb_test_data)\n",
    "accuracy = accuracy_score(imdb_test_target, predicted)\n",
    "print(\"The accuracy score for logistic regression on the IMDB dataset is\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9ba0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
